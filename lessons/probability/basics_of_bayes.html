<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>22&nbsp; Basics of Bayesian modeling – DataSAI for Neuroscience Summer School</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../lessons/probability/stats_ml_ai.html" rel="next">
<link href="../../lessons/probability/bayesian_modeling.html" rel="prev">
<link href="../../logo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-a185852c63625fd9ffbdc57047c9a77e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-4002b068bac9de7bdc97299d920a6be6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../lessons/probability/bayesian_modeling.html">Bayesian modeling and inference</a></li><li class="breadcrumb-item"><a href="../../lessons/probability/basics_of_bayes.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Basics of Bayesian modeling</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="../../"></a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About DataSAI for Neuroscience Summer School 2025</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/exploratory/polars.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Polars and split-apply-combine</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/intro_to_polars.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to data frames and Polars</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/tidy_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Tidy data and split-apply-combine</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/polars_for_pandas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Polars for Pandas users</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/exploratory/plotting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data display</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/plotting_with_bokeh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Making plots with Bokeh</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/iqplot.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">High level plotting with iqplot</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/styling_bokeh.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Styling Bokeh plots</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/overplotting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Dealing with overplotting</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/exploratory/file_formats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data file formats</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/matlab_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Example file format: MAT-files</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/tdt_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Example file format: TDT files</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/exploratory/nwb_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Example file format: NWB files</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/probability/probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability: The foundation for generative modeling</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/probability/probability_logic_of_science.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Probability as the logic of science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/probability/probability_distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Probability distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/probability/entropy_and_kullback_leibler.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Entropy and the Kullback-Leibler divergence</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/sampling/sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sampling out of probability distributions</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/random_number_generation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random number generation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/rng_with_numpy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Random number generation using Numpy</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/sampling/simulation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simulating generative distributions</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/luria_delbruck.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Simulating the Luria-Delbrück distribution</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/lif.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">The noisy leaky integrate-and-fire model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/nonhomogeneous_poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Modeling nonhomogeneous Poisson spiking</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/sampling/mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Markov chain Monte Carlo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/basics_of_mcmc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">The basics of Markov chain Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/stan_hello_world.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">“Hello, world” —Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/sampling/nonhomogeneous_poisson_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Nonhomogeneous Poisson process arrival times with Stan</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/probability/bayesian_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bayesian modeling and inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/probability/basics_of_bayes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Basics of Bayesian modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/probability/stats_ml_ai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">What about machine learning and artificial intelligence?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/probability/bayes_model_for_learning_and_cognition.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Bayes's theorem as a model for learning</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../lessons/optimization/optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Summarizing posterior distributions with maxima</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/optimization/optimization_basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Bayesian approach to parameter estimation by optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/optimization/mm_algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Minorize-maximize algorithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/optimization/em_algorithm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">The expectation-maximization (EM) algorithm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/optimization/em_gmm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">EM applied to a Gaussian mixture model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/optimization/em_gmm_example.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">An example application of the EM algorithm to a Gaussian mixture model</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/optimization/kmeans.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">K-means clustering</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false">
 <span class="menu-text">Principal component analysis and related models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/pca_and_related/heuristic_pca.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Principal component analysis: A heuristic approach</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/pca_and_related/factor_analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Factor analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/pca_and_related/pca_nmf_as_fa.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Special cases of factor analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../lessons/HMM/hmm_gaussian_emission.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Hidden Markov models</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Notation</span></span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false">
 <span class="menu-text">Computing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/setting_up_python_computing_environment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Configuring your computer to use Python for scientific computing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/hello_world.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Hello, world.</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/variables_operators_types.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Variables, operators, and types</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/lists_and_tuples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Lists and tuples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/iteration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Iteration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/intro_to_functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Introduction to functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/string_methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">String methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/dictionaries.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Dictionaries</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/comprehensions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Comprehensions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/packages_and_modules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Packages and modules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/exceptions_and_error_handling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Errors and exception handling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/file_io.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">File I/O</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../appendices/python/intro_to_numpy_and_scipy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Introduction to Numpy and Scipy</span></span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">In this section</h2>
   
  <ul class="collapse">
  <li><a href="#tasks-of-bayesian-modeling" id="toc-tasks-of-bayesian-modeling" class="nav-link active" data-scroll-target="#tasks-of-bayesian-modeling"><span class="header-section-number">23</span> Tasks of Bayesian modeling</a>
  <ul class="collapse">
  <li><a href="#model-building" id="toc-model-building" class="nav-link" data-scroll-target="#model-building"><span class="header-section-number">23.1</span> Model building</a></li>
  <li><a href="#making-sense-of-the-posterior" id="toc-making-sense-of-the-posterior" class="nav-link" data-scroll-target="#making-sense-of-the-posterior"><span class="header-section-number">23.2</span> Making sense of the posterior</a></li>
  </ul></li>
  <li><a href="#bayesian-modeling-example-parameter-estimation-from-repeated-measurements" id="toc-bayesian-modeling-example-parameter-estimation-from-repeated-measurements" class="nav-link" data-scroll-target="#bayesian-modeling-example-parameter-estimation-from-repeated-measurements"><span class="header-section-number">24</span> Bayesian modeling example: parameter estimation from repeated measurements</a>
  <ul class="collapse">
  <li><a href="#the-likelihood" id="toc-the-likelihood" class="nav-link" data-scroll-target="#the-likelihood"><span class="header-section-number">24.1</span> The likelihood</a></li>
  <li><a href="#the-normal-distribution" id="toc-the-normal-distribution" class="nav-link" data-scroll-target="#the-normal-distribution"><span class="header-section-number">24.2</span> The Normal distribution</a></li>
  <li><a href="#the-likelihood-revisited-and-another-parameter" id="toc-the-likelihood-revisited-and-another-parameter" class="nav-link" data-scroll-target="#the-likelihood-revisited-and-another-parameter"><span class="header-section-number">24.3</span> The likelihood revisited: and another parameter</a></li>
  <li><a href="#choice-of-prior" id="toc-choice-of-prior" class="nav-link" data-scroll-target="#choice-of-prior"><span class="header-section-number">24.4</span> Choice of prior</a></li>
  <li><a href="#succinctly-stating-the-model" id="toc-succinctly-stating-the-model" class="nav-link" data-scroll-target="#succinctly-stating-the-model"><span class="header-section-number">24.5</span> Succinctly stating the model</a></li>
  </ul></li>
  <li><a href="#choosing-likelihoods" id="toc-choosing-likelihoods" class="nav-link" data-scroll-target="#choosing-likelihoods"><span class="header-section-number">25</span> Choosing likelihoods</a></li>
  <li><a href="#choosing-priors" id="toc-choosing-priors" class="nav-link" data-scroll-target="#choosing-priors"><span class="header-section-number">26</span> Choosing priors</a>
  <ul class="collapse">
  <li><a href="#uniform-priors" id="toc-uniform-priors" class="nav-link" data-scroll-target="#uniform-priors"><span class="header-section-number">26.1</span> Uniform priors</a></li>
  <li><a href="#jeffreys-priors" id="toc-jeffreys-priors" class="nav-link" data-scroll-target="#jeffreys-priors"><span class="header-section-number">26.2</span> Jeffreys priors</a></li>
  <li><a href="#example-jeffreys-priors" id="toc-example-jeffreys-priors" class="nav-link" data-scroll-target="#example-jeffreys-priors"><span class="header-section-number">26.3</span> Example Jeffreys priors</a></li>
  <li><a href="#why-not-use-jeffreys-priors" id="toc-why-not-use-jeffreys-priors" class="nav-link" data-scroll-target="#why-not-use-jeffreys-priors"><span class="header-section-number">26.4</span> Why not use Jeffreys priors?</a></li>
  <li><a href="#weakly-informative-priors" id="toc-weakly-informative-priors" class="nav-link" data-scroll-target="#weakly-informative-priors"><span class="header-section-number">26.5</span> Weakly informative priors</a></li>
  <li><a href="#conjugate-priors" id="toc-conjugate-priors" class="nav-link" data-scroll-target="#conjugate-priors"><span class="header-section-number">26.6</span> Conjugate priors</a></li>
  <li><a href="#the-bet-the-farm-method-of-specifying-weakly-informative-priors" id="toc-the-bet-the-farm-method-of-specifying-weakly-informative-priors" class="nav-link" data-scroll-target="#the-bet-the-farm-method-of-specifying-weakly-informative-priors"><span class="header-section-number">26.7</span> The bet-the-farm method of specifying weakly informative priors</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../lessons/probability/bayesian_modeling.html">Bayesian modeling and inference</a></li><li class="breadcrumb-item"><a href="../../lessons/probability/basics_of_bayes.html"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Basics of Bayesian modeling</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-basics-of-bayesian-modeling" class="quarto-section-identifier"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Basics of Bayesian modeling</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="tasks-of-bayesian-modeling" class="level1" data-number="23">
<h1 data-number="23"><span class="header-section-number">23</span> Tasks of Bayesian modeling</h1>
<p>We have learned about Bayes’s theorem as a way to update a hypothesis in light of new data. We use the word “hypothesis” very loosely here. Remember, in the Bayesian view, probability can describe the plausibility of any proposition. The value of a parameter is such a proposition, and we will apply the term “hypothesis” to parameter values as well. Again for concreteness, we will consider the problem of parameter estimation here, where <span class="math inline">\(\theta\)</span> will denote a set of parameter values, our “hypothesis” in this context.</p>
<p>As we have seen, our goal is to obtain updated knowledge about <span class="math inline">\(\theta\)</span> as codified in the posterior distribution, <span class="math inline">\(g(\theta \mid y)\)</span>, where <span class="math inline">\(y\)</span> is the data we measured. Bayes's theorem gives us access to this distribution.</p>
<p><span class="math display">\[\begin{align}
g(\theta \mid y) = \frac{f(y\mid\theta)\,g(\theta)}{f(y)}.
\end{align}\]</span></p>
<p>Note, though, that the split between the likelihood and prior need not be explicit. In fact, when we study hierarchical models, we will see that it is not obvious how to unambiguously define the likelihood and prior. In order to do Bayesian inference, we really only need to specify the joint distribution, <span class="math inline">\(\pi(y,\theta) = f(y\mid\theta)\,g(\theta)\)</span>, which is the product of the likelihood and prior. The posterior can also be written as</p>
<p><span class="math display">\[\begin{align}
g(\theta \mid y) = \frac{\pi(y,\theta)}{f(y)}.
\end{align}\]</span></p>
<p>Because the evidence, <span class="math inline">\(f(y)\)</span>, is computed from the likelihood and prior (or more generally from the joint distribution) as</p>
<p><span class="math display">\[\begin{align}
f(y) = \int\mathrm{d}\theta\,\pi(y,\theta) = \int\mathrm{d}\theta\,f(y\mid\theta)\,g(\theta),
\end{align}\]</span></p>
<p>the posterior is fully specified by the joint distribution. In light of this, there are two broad tasks in Bayesian modeling.</p>
<ol type="1">
<li>Define the joint distribution <span class="math inline">\(\pi(y,\theta)\)</span>, most commonly specified as a likelihood and prior.</li>
<li>Make sense of the resulting posterior.</li>
</ol>
<section id="model-building" class="level2" data-number="23.1">
<h2 data-number="23.1" class="anchored" data-anchor-id="model-building"><span class="header-section-number">23.1</span> Model building</h2>
<p>Step (1) is the task of <strong>model building</strong>, or modeling. This is typically done by first specifying a likelihood, <span class="math inline">\(f(y\mid \theta)\)</span>. This is a generative distribution in that it gives the full description of how data <span class="math inline">\(y\)</span> are generated given parameters <span class="math inline">\(\theta\)</span>. Importantly, included in the likelihood is a theoretical model.</p>
<p>After the likelihood is constructed we need to specify prior distributions for the parameters of the likelihood. That is, the likelihood, as a model of the generative process, prescribes what the parameters <span class="math inline">\(\theta\)</span> of the model are (but not their values). Knowing what parameters we need to write priors for, we go about codifying what we know about the parameters before we do the experiment as prior probabilities.</p>
<p>So, the process of model building involves prescribing a likelihood that describes the process of data generation. The parameters to be estimated are defined in the likelihood, and from these we construct priors. We will discuss <em>how</em> we go about building likelihoods and priors in this lecture and subsequently in the course.</p>
<section id="the-role-of-the-prior" class="level3" data-number="23.1.1">
<h3 data-number="23.1.1" class="anchored" data-anchor-id="the-role-of-the-prior"><span class="header-section-number">23.1.1</span> The role of the prior</h3>
<p>As an aside, I pause to note that there may be some philosophical issues with this approach. I think Gelman, Simpson, and Betancourt clearly stated the dilemma in <a href="https://doi.org/10.3390/e19100555">their 2017 paper</a> with the apt title, "The Prior Can Often Only Be Understood in the Context of the Likelihood" (emphasis added by me).</p>
<blockquote class="blockquote">
<p>One can roughly speak of two sorts of Bayesian analyses. In the first sort, the Bayesian formalism can be taken literally: a researcher starts with a prior distribution about some externally defined quantity, perhaps some physical parameter or the effect of some social intervention, and then he or she analyzes data, leading to an updated posterior distribution. Here, the prior can be clearly defined, not just before the data are observed but <em>before the experiment has even been considered</em>. ...[W]e are concerned with a second sort of analysis, what might be patterned Bayesian analysis using default priors, in which a researcher gathers data and forms a model that includes various unknown parameters and then needs to set up a prior distribution to continue with Bayesian inference. This latter approach is standard in Bayesian workflow...</p>
<p>One might say that what makes a prior a prior, rather than simply a probability distribution, is that it is destined to be paired with a likelihood. That is, the Bayesian formalism requires that a prior distribution be updated into a posterior distribution based on new data.</p>
</blockquote>
<p>We are taking the second approach the Gelman, Simpson, and Betancourt laid out; the approach where the prior is in service to a likelihood. We are trying to model a data generation process, which requires a likelihood to even identify what the parameters are that require priors.</p>
</section>
</section>
<section id="making-sense-of-the-posterior" class="level2" data-number="23.2">
<h2 data-number="23.2" class="anchored" data-anchor-id="making-sense-of-the-posterior"><span class="header-section-number">23.2</span> Making sense of the posterior</h2>
<p>If we can write down the likelihood and prior, or more generally the joint distribution <span class="math inline">\(\pi(y,\theta)\)</span>, we theoretically have the full posterior distribution. We have seen in our previous lessons that for simple models involving one parameter, we can often plot the posterior distribution in order to interpret it. When we have two parameters, it becomes more challenging to make plots, but it can be done. Things get more and more complicated as the number of parameters grow, a manifestation of the curse of dimensionality.</p>
<p>At the heart of the technical challenges of Bayesian inference is how to make sense of the posterior. Plotting it is useful, but in almost all cases, we wish to compute <strong>expectations</strong> of the posterior. An expectation computed from the posterior is of the form</p>
<p><span class="math display">\[\begin{align}
\langle \xi \rangle = \int \mathrm{d}\theta'\, \xi(\theta')\,g(\theta'\mid y),
\end{align}\]</span></p>
<p>with the integral replaced by a sum for discrete <span class="math inline">\(\theta\)</span>. As an example, we may have <span class="math inline">\(\theta = (\theta_1, \theta_2)\)</span> and we wish to compute a marginal posterior <span class="math inline">\(g(\theta_2 \mid y)\)</span>. In this case, <span class="math inline">\(\xi(\theta') = \delta(\theta_2' - \theta_2)\)</span>, a Dirac delta function, and</p>
<p><span class="math display">\[\begin{align}
g(\theta_2 \mid y) = \int \mathrm{d}\theta_1 \, g(\theta_1, \theta_2 \mid y).
\end{align}\]</span></p>
<p>So, making sense of the posterior typically involves computing integrals (or sums).</p>
<p>In addition to using conjagacy, which we have already discussed, in coming lessons, we will explore a few ways to do this.</p>
<ol type="1">
<li>Finding the values of the parameters <span class="math inline">\(\theta\)</span> that maximize the posterior and then approximating the posterior as locally Normal. This involves a numerical optimization calculation to find the maximizing parameters, and then uses known analytical results about multivariate Normal distributions to automatically compute the integrals (without actually having to do integration!).</li>
<li><em>Sampling</em> out of the posterior. As you saw in an earlier homework, we can perform marginalization, and indeed most other integrals, by sampling. The trick is sampling out of the posterior, which is not as easy as the sampling you have done so far. We resort to using the sampling method called <strong>Markov chain Monte Carlo</strong> (MCMC) to do it.</li>
<li>Performing and optimization to find which distribution in a family of distributions most closely approximates the posterior, and then use this approximate distribution as a substitute for the posterior. We choose the family of candidate distributions to have nice properties, known analytical results, ease of sampling, etc., thereby making exploration of the posterior much easier than by full MCMC. This is called <strong>variational inference</strong>.</li>
</ol>
<p>For the rest of this lecture, though, we will focus on model building.</p>
</section>
</section>
<section id="bayesian-modeling-example-parameter-estimation-from-repeated-measurements" class="level1" data-number="24">
<h1 data-number="24"><span class="header-section-number">24</span> Bayesian modeling example: parameter estimation from repeated measurements</h1>
<p>We will consider one of the simplest examples of parameter estimation, and one that comes up often in research applications. Let’s say we repeat a measurement many times. This could be beak depths of finches, fluorescence intensity in a cell, etc. The possibilities abound. To have a concrete example in mind for this example, let’s assume we are measuring the length of <em>C. elegans</em> eggs.</p>
<p>Our measurements of <em>C. elegans</em> egg length are <span class="math inline">\(y \equiv\{y_1, y_2, \ldots y_n\}\)</span>. We will ambiguously define a parameter of interest to be <span class="math inline">\(\mu\)</span>, the typical egg length. We will sharpen our definition of this parameter through specification of the likelihood.</p>
<p>We wish to calculate <span class="math inline">\(g(\mu  \mid y)\)</span>, the posterior probability density function for the parameter <span class="math inline">\(\mu\)</span>, given the data. Values of <span class="math inline">\(\mu\)</span> for which the posterior probability density is high are more probable (that is, more plausible) than those for which is it low. The posterior <span class="math inline">\(g(\mu \mid y)\)</span> codifies our knowledge about <span class="math inline">\(\mu\)</span> in light of our data <span class="math inline">\(y\)</span>.</p>
<p>To compute the posterior, we use Bayes’s theorem.</p>
<p><span class="math display">\[\begin{aligned}
g(\mu \mid y) = \frac{f(y\mid \mu)\,g(\mu)}{f(y)}.
\end{aligned}\]</span></p>
<p>Since the evidence, <span class="math inline">\(f(y)\)</span> does not depend on the parameter of interest, <span class="math inline">\(\mu\)</span>, it is really just a normalization constant, so we do not need to consider it explicitly at this stage. Specification of the likelihood and prior is sufficient for the posterior, since we must have</p>
<p><span class="math display">\[\begin{aligned}
f(y) = \int \mathrm{d}\mu \,f(y\mid \mu)\,g(\mu)
\end{aligned}\]</span></p>
<p>to ensure normalization of the posterior <span class="math inline">\(g(\mu \mid y)\)</span>. So, we have just to specify the likelihood <span class="math inline">\(f(y\mid \mu)\)</span> and the prior <span class="math inline">\(g(\mu)\)</span>. We begin with the likelihood.</p>
<section id="the-likelihood" class="level2" data-number="24.1">
<h2 data-number="24.1" class="anchored" data-anchor-id="the-likelihood"><span class="header-section-number">24.1</span> The likelihood</h2>
<p>To specify the likelihood, we have to ask what we expect from the data, given a value of <span class="math inline">\(\mu\)</span>. If every egg has exactly the same length and there are no errors or confounding factors at all in our measurements, we expect <span class="math inline">\(y_i = \mu\)</span> for all <span class="math inline">\(i\)</span>. In this case</p>
<p><span class="math display">\[\begin{aligned}
g(y\mid \mu) = \prod_{i=1}^n\delta(y_i - \mu),
\end{aligned}\]</span></p>
<p>the product of Dirac delta functions. Of course, this is really never the case. There will be natural variation in egg length and some errors in measurement and/or the system has variables that confound the measurement. What, then should we choose for our likelihood?</p>
<p>That choice is of course dependent the story/theoretical modeling behind data generation. For our purposes here, we shall assume our data are generated from a Normal likelihood. Since this distribution gets heavy use, I will pause here to talk a bit more about it.</p>
</section>
<section id="the-normal-distribution" class="level2" data-number="24.2">
<h2 data-number="24.2" class="anchored" data-anchor-id="the-normal-distribution"><span class="header-section-number">24.2</span> The Normal distribution</h2>
<p>A univariate Normal (also known as Gaussian), probability distribution has a probability density function (PDF) of</p>
<p><span class="math display">\[\begin{aligned}
f(y \mid \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\,
\exp\left[-\frac{(y - \mu)^2}{2\sigma^2}\right].
\end{aligned}\]</span></p>
<p>The parameter <span class="math inline">\(\mu\)</span> is a location parameter and in the case of the Normal distribution is called the mean and <span class="math inline">\(\sigma\)</span> is a scale parameter and is called the standard deviation in this case. The square of this scale parameter is referred to as the variance. Importantly (and confusingly), the terms "mean," "standard deviation," and "variance" in this context are <em>names of parameters</em> of the distribution; they are not what you compute directly from data as plug-in estimates. We will therefore use the terms location parameter and scale parameter, though the terms mean and standard deviation are used very widely in the literature.</p>
<p>The <strong>central limit theorem</strong> says that any quantity that emerges from a large number of subprocesses tends to be Normally distributed, provided none of the subprocesses is very broadly distributed. We will not prove this important theorem, but we make use of it when choosing likelihood distributions based on the stories behind the generative process. Indeed, in the simple case of estimating a single parameter where many processes may contribute to noise in the measurement, the Normal distribution is a good choice for a likelihood.</p>
<p>More generally, the <strong>multivariate Normal distribution</strong> for <span class="math inline">\(y = (y_1, y_2, \cdots, y_n)^\mathsf{T}\)</span> is</p>
<p><span class="math display">\[\begin{aligned}
f(y \mid \boldsymbol{\mu}, \mathsf{\Sigma}) = (2\pi)^{-\frac{n}{2}} \left(\det \mathsf{\Sigma}\right)^{-\frac{1}{2}}\,
\exp\left[-\frac{1}{2}(y - \boldsymbol{\mu})^\mathsf{T}\cdot \mathsf{\Sigma}^{-1}\cdot(y - \boldsymbol{\mu})\right],
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu} = (\mu_1, \mu_2,\ldots, \mu_n)^\mathsf{T}\)</span> is an array of location parameters. The parameter <span class="math inline">\(\mathsf{\Sigma}\)</span> is a symmetric positive definite matrix called the <strong>covariance matrix</strong>. If off-diagonal entry <span class="math inline">\(\Sigma_{ij}\)</span> is nonzero, then <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> are correlated (or anticorrelated). In the case where all <span class="math inline">\(y_i\)</span> are independent, all off-diagonal terms in the covariance matrix are zero, and the multivariate Normal distribution reduces to</p>
<p><span class="math display">\[\begin{aligned}
f(y \mid \boldsymbol{\mu}, \boldsymbol{\sigma}) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma_i^2}}\,
\exp\left[-\frac{(y_i - \mu_i)^2}{2\sigma_i^2}\right],
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\sigma^2_i\)</span> is the <span class="math inline">\(i`th entry along the diagonal
of the covariance matrix. This is the variance associated with
measurement :math:`i\)</span>. So, if all independent measurements <span class="math inline">\(y_i\)</span> have the same location and scale parameters, which is to say that the measurements are <strong>independent and identically distributed</strong> (i.i.d.), the multi-dimensional Gaussian reduces to</p>
<p><span class="math display">\[\begin{aligned}
f(y \mid \mu, \sigma) = \left(\frac{1}{2\pi \sigma^2} \right)^{-\frac{n}{2}}\,
\exp\left[-\frac{1}{2\sigma^2}\,\sum_{i=1}^n (y_i - \mu)^2\right].
\end{aligned}\]</span></p>
</section>
<section id="the-likelihood-revisited-and-another-parameter" class="level2" data-number="24.3">
<h2 data-number="24.3" class="anchored" data-anchor-id="the-likelihood-revisited-and-another-parameter"><span class="header-section-number">24.3</span> The likelihood revisited: and another parameter</h2>
<p>For the purposes of this demonstration of parameter estimation, we assume the Normal distribution is a good choice for our likelihood for repeated measurements (as it often is). We have to decide how the measurements are related to specify how many entries in the covariance matrix we need to specify as parameters. It is often the case that the measurements are i.i.d, so that only a single mean and variance are specified. So, we choose our likelihood to be</p>
<p><span class="math display">\[\begin{aligned}
f(y\mid \mu, \sigma) = \left(\frac{1}{2\pi \sigma^2} \right)^{\frac{n}{2}}\,
\exp\left[-\frac{1}{2\sigma^2}\,\sum_{i=1}^n (y_i - \mu)^2\right].
\end{aligned}\]</span></p>
<p>By choosing this as our likelihood, we are saying that we expect our measurements to have a well-defined mean <span class="math inline">\(\mu\)</span> with a spread described by the variance, <span class="math inline">\(\sigma^2\)</span>.</p>
<p>But wait a minute; we had a single parameter, <span class="math inline">\(\mu\)</span>, that we sought to estimate, and now we now have another parameter, <span class="math inline">\(\sigma\)</span>, beyond the one we’re trying to measure. So, our statistical model has <em>two</em> parameters, and Bayes’s theorem now reads</p>
<p><span class="math display">\[\begin{aligned}
g(\mu, \sigma \mid y) = \frac{f(y\mid \mu, \sigma)\,g(\mu, \sigma)}
{f(y)}.
\end{aligned}\]</span></p>
<p>After we compute the posterior, we can still find the posterior probability distribution we are after by marginalizing.</p>
<p><span class="math display">\[\begin{aligned}
g(\mu\mid y) = \int_0^\infty \mathrm{d}\sigma\,g(\mu, \sigma \mid y).
\end{aligned}\]</span></p>
</section>
<section id="choice-of-prior" class="level2" data-number="24.4">
<h2 data-number="24.4" class="anchored" data-anchor-id="choice-of-prior"><span class="header-section-number">24.4</span> Choice of prior</h2>
<p>Now that we have defined a likelihood, we know what the parameters are and we can define a prior, <span class="math inline">\(g(\mu, \sigma)\)</span>. As is often the case, we assume <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are independent of each other, so that</p>
<p><span class="math display">\[\begin{aligned}
g(\mu, \sigma) = g(\mu)\,g(\sigma).
\end{aligned}\]</span></p>
<p>How might we choose prior distributions for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>? Remember, the prior probability distribution captures what we know about the parameter before we measure data. For the current example of <em>C. elegans</em> eggs, we can guess that the egg length should be about 50&nbsp;µm, but we are not too sure about this. So, we will make the prior distribution broad; we take <span class="math inline">\(g(\mu)\)</span> to be Normal with a location parameter of 50&nbsp;µm, but a scale parameter of 20&nbsp;µm. That is,</p>
<p><span class="math display">\[\begin{aligned}
g(\mu) = \frac{1}{\sqrt{2\pi \sigma_\mu^2}}\,\exp\left[-\frac{(\mu-\mu_\mu)^2}{2\sigma^2}\right],
\end{aligned}\]</span></p>
<p>with <span class="math inline">\(\mu_\mu = 50\)</span>&nbsp;µm and <span class="math inline">\(\sigma_\mu = 20\)</span>&nbsp;µm. This means that getting a very tiny egg length of, say, 10&nbsp;µm is unlikely, as is a very large egg of 90&nbsp;µm.</p>
<p>For <span class="math inline">\(g(\sigma)\)</span>, we might think that the egg length may vary about five or ten microns (approximately 10-20% of our guessed egg size), but not much more than that. We could again choose a Normal prior, with</p>
<p><span class="math display">\[\begin{aligned}
g(\sigma) = \frac{1}{\sqrt{2\pi \sigma_\sigma^2}}\,\exp\left[-\frac{(\mu-\mu_\sigma)^2}{2\sigma_\sigma^2}\right],\end{aligned}\]</span></p>
<p>with <span class="math inline">\(\mu_\sigma = 5\)</span>&nbsp;µm and <span class="math inline">\(\sigma_\sigma = 2\)</span>&nbsp;µm.</p>
<p>In this case, we have the obvious issue that there is nonzero probability that <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\sigma\)</span> could be negative, which we know is respectively unphysical or mathematically disallowed. We could refine our prior distribution to make sure this does not happen. With any approach we choose, the prior should roughly match what we would sketch on a piece of paper and cover any reasonable parameter values and exclude any that are unreasonable (or unphysical).</p>
</section>
<section id="succinctly-stating-the-model" class="level2" data-number="24.5">
<h2 data-number="24.5" class="anchored" data-anchor-id="succinctly-stating-the-model"><span class="header-section-number">24.5</span> Succinctly stating the model</h2>
<p>Our model is complete, which means that we have now completely specified the posterior. We can write it out. First, defining the parametrization of the priors.</p>
<p><span class="math display">\[\begin{aligned}
\begin{align}
&amp;\mu_\mu = 50 \text{ microns} \nonumber \\[1em]
&amp;\sigma_\mu = 20 \text{ microns} \nonumber \\[1em]
&amp;\mu_\sigma = 5 \text{ microns} \nonumber \\[1em]
&amp;\sigma_\sigma = 2 \text{ microns}.
\end{align}
\end{aligned}\]</span></p>
<p>Then, the posterior is</p>
<p><span class="math display">\[\begin{aligned}
\begin{aligned}
g(\mu, \sigma\mid y) = &amp;\;\frac{1}{f(y)}\,\left\{\left(\frac{1}{2\pi \sigma^2} \right)^{\frac{n}{2}}\,
\exp\left[-\frac{1}{2\sigma^2}\,\sum_{i=1}^n (y_i - \mu)^2\right] \right.\nonumber\\
&amp;\;\;\;\;\;\;\;\;\;\;\;\times \, \frac{1}{\sqrt{2\pi \sigma_\mu^2}}\,\exp\left[-\frac{(\mu-\mu_\mu)^2}{2 \sigma_\mu^2}\right] \nonumber\\
&amp;\;\;\;\;\;\;\;\;\;\;\;\times\,\left.\frac{1}{\sqrt{2\pi \sigma_\sigma^2}}\,\exp\left[-\frac{(\sigma-\mu_\sigma)^2}{2 \sigma_\sigma^2}\right]\right\},
\end{aligned}
\end{aligned}\]</span></p>
<p>with</p>
<p><span class="math display">\[\begin{aligned}
f(y) = \int \mathrm{d}\mu\,\int\mathrm{d}\sigma\, \{\text{term in braces in the above equation}\}.
\end{aligned}\]</span></p>
<p>Oh my, this is a mess, even for this simple model! Even though we have the posterior, it is very hard to make sense of it. Essentially the rest of the course involved making sense of the posterior, which is the challenge. It turns out that writing it down is relatively easy.</p>
<p>One of the first things we can do to make sense of our model, and also to specify it, is to use a shorthand for model specification. First of all, we do not need to specify the evidence, since it is always given by integrating the likelihood and prior; that is by fully marginalizing the likelihood. So, we will always omit its specification. Now, we would like to have a notation for stating the likelihood and prior. English works well.</p>
<blockquote class="blockquote">
<ul>
<li>The parameter <span class="math inline">\(\mu\)</span> is Normally distributed with location parameter 50&nbsp;µm and scale parameter 20&nbsp;µm.</li>
<li>The parameter <span class="math inline">\(\sigma\)</span> is Normally distributed with location parameter 5&nbsp;µm and scale parameter 2&nbsp;µm.</li>
<li>The egg lengths are i.i.d. and are Normally distributed with location parameter <span class="math inline">\(\mu\)</span> and scale parameter <span class="math inline">\(\sigma\)</span>.</li>
</ul>
</blockquote>
<p>This is much easier to understand. We can write this with a convenient, and self evident, shorthand.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[\begin{aligned}
\begin{aligned}
&amp;\mu \sim \text{Norm}(\mu_\mu, \sigma_\mu),\\[1em]
&amp;\sigma \sim \text{Norm}(\mu_\sigma, \sigma_\sigma),\\[1em]
&amp;y_i \sim \text{Norm}(\mu, \sigma) \;\forall i.
\end{aligned}
\end{aligned}\]</span></p>
<p>Here, the symbol <span class="math inline">\(\sim\)</span> may be read as “is distributed as.” The above three lines are completely sufficient to specify our model. Because we will be using a probabilistic programming language in practice, we will almost never need to code up any nasty mathematical expressions in our modeling and we can express a model as we have done here.</p>
<hr>
</section>
</section>
<section id="choosing-likelihoods" class="level1" data-number="25">
<h1 data-number="25"><span class="header-section-number">25</span> Choosing likelihoods</h1>
<p>In our example of model building for measurements of <em>C. elegans</em> egg lengths, we chose a Normal likelihood for the egg lengths. We did so because the <strong>story</strong> of the repeated measurements matched that of the Normal distribution via the central limit theorem. When a measurement is the result of many processes, none of which has an enormous variance, the values of the measurement is Normally distributed.</p>
<p>This method of choosing a likelihood amounts to <strong>story matching</strong>. The idea is that we describe the data generation process with a story. We then find a distribution that describes the outcome of the story.</p>
<p>For example, one might perform a single molecule experiment measuring individual binding events of a ligand to a receptor and record the time between binding events. A possible model story for this is that the binding events are all independent and without memory; that is their timing does not depend on previous binding events. This means that we can model binding events as a <a href="https://en.wikipedia.org/wiki/Poisson_point_process">Poisson process</a> and are interested in the timing between arrivals (binding events) of the Poisson process. This story matches the story of the <a href="http://bois.caltech.edu/distribution_explorer/continuous/exponential.html">Exponential distribution</a>, so we would use it for our likelihood.</p>
<p>The procedure of story matching is an important part of Bayesian modeling. In a great many cases, there exists a well-known, named distribution that matches the story you are using to model the generation of your data. In cases where no such distribution exists, or you do not know about it, you need to derive a PDF or PMF matching your story, which can be challenging. It is therefore well worth the time investment to know about distributions that can be useful in modeling. You should read the contents of the <a href="https://distribution-explorer.github.io/">Distribution Explorer</a> to get yourself familiar with named distributions. This will greatly facilitate choosing likelihoods (and priors).</p>
</section>
<section id="choosing-priors" class="level1" data-number="26">
<h1 data-number="26"><span class="header-section-number">26</span> Choosing priors</h1>
<p>While choosing likelihoods often amounts to story matching, choosing priors can be more subtle and challenging. In our example of model building for measurements of <em>C. elegans</em> egg lengths, we assumed Normal priors for the two parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. We did that because we felt that it best codified in probabilistic terms our knowledge of those parameters before seeing the data. That is one of many ways we can go about choosing priors. In fact, choice of prior is a major topic of (often heated) discussion about how best to go about Bayesian modeling. Some believe that the fact you have to specify a prior in the Bayesian framework invalidates Bayesian inference entirely because it necessarily introduces a modeler's bias into the analysis.</p>
<p>Among the many approaches to choose priors are choices of uniform priors, Jeffreys priors, weakly informative priors, conjugate priors, maximum entropy priors, Bernardo's reference priors, and others. We will discuss the first four of these, eventually advocating for weakly informative priors.</p>
<section id="uniform-priors" class="level2" data-number="26.1">
<h2 data-number="26.1" class="anchored" data-anchor-id="uniform-priors"><span class="header-section-number">26.1</span> Uniform priors</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Principle_of_indifference">principle of insufficient reason</a> is an old rule for assigning probability to events or outcomes. It simply says that if we do not know anything about a set of outcomes, then every possible outcome should be assigned equal probability. Thus, we assume that the prior is flat, or uniform.</p>
<p>This notion is quite widely used. In fact, if we attempt to summarize the posterior by a single point in parameter space, namely where the posterior is maximal, and we chose uniform priors for all parameters, we get estimates for the parameters that are the same as if we performed a maximum likelihood estimate in a frequentist approach.</p>
<p>However, using uniform priors has a great many problems. I discuss but a few of them here.</p>
<ol type="1">
<li>If a parameter may take any value along the number line, or any positive value, then a uniform prior is not normalizable. This is because <span class="math inline">\(\int_0^\infty \mathrm{d}\theta\,(\text{constant})\)</span> diverges. Such a prior is said to be an <strong>improper prior</strong>, since it is not a true probability distribution (nor probability in the discrete case). This means that they cannot actually describe prior knowledge of a parameter value as encoded by the machinery of probability.</li>
<li>We can remedy point (1) by specifying bounds on the prior. This is no longer a uniform prior, though, since we are saying that parameter values between the bounds are infinitely more likely than those outside of the bounds. At a small distance <span class="math inline">\(\epsilon\)</span> from an upper bound, for example, we have <span class="math inline">\(\theta - \epsilon\)</span> being infinitely more likely than <span class="math inline">\(\theta + \epsilon\)</span>, which does not make intuitive sense.</li>
<li>Surely priors cannot be uniform. For example, if we were trying to measure the speed of a kinesin motor, we know that it does not go faster than the speed of light (because nothing goes faster than the speed of light). With an improper uniform prior, we are saying that before we see and experiment, we believe that kinesin is more likely to go faster than the speed of light than it is to move at a micron per second. This is absurd. We will deal with this issue when discussing weakly informative priors.</li>
<li>A primary criticism from Fisher and his contemporaries was that the way you choose to parametrize a model can affect how a uniform prior transforms. We illustrate this problem and its resolution when we talk about Jeffreys priors next.</li>
</ol>
<p>In summary, uniform priors, while widely used, and in fact used in the early homeworks of this course, are a pathologically bad idea. (Note, though, that this is still subject to debate, and many respected researchers do not agree with this assessment.)</p>
</section>
<section id="jeffreys-priors" class="level2" data-number="26.2">
<h2 data-number="26.2" class="anchored" data-anchor-id="jeffreys-priors"><span class="header-section-number">26.2</span> Jeffreys priors</h2>
<p>Fisher and others complained that application of the principle of insufficient reason to choose uniform priors resulted in different effects based on parametrization of a model. To make this more concrete consider the example of a one-dimensional Normal likelihood. The probability density function is</p>
<p><span class="math display">\[\begin{align}
f(y\mid\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}\,\mathrm{e}^{-(y-\mu)^2/2\sigma^2}.
\end{align}\]</span></p>
<p>Instead of parametrizing by <span class="math inline">\(\sigma\)</span>, we could have instead chosen to parametrize with <span class="math inline">\(\tau \equiv 1/\sigma\)</span>, giving a PDF of</p>
<p><span class="math display">\[\begin{align}
f(y\mid\mu,\tau) = \frac{\tau}{\sqrt{2\pi}}\,\mathrm{e}^{-\tau^2(y-\mu)^2/2}.
\end{align}\]</span></p>
<p>Now, if we choose a uniform prior for <span class="math inline">\(\sigma\)</span>, we should also expect a uniform prior for <span class="math inline">\(\tau\)</span>. But this is not the case. Recall the change of of variables formula.</p>
<p><span class="math display">\[\begin{align}
g(\tau) = \left|\frac{\mathrm{d}\sigma}{\mathrm{d}\tau}\right|g(\sigma) = \frac{\text{constant}}{\tau^2},
\end{align}\]</span></p>
<p>since <span class="math inline">\(g(\sigma) = \text{constant}\)</span> for a uniform prior and <span class="math inline">\(|\mathrm{d}\sigma/\mathrm{d}\tau| = 1/\tau^2\)</span>. So, if we parametrize the likelihood with <span class="math inline">\(\tau\)</span> instead of <span class="math inline">\(\sigma\)</span>, the priors are inconsistent. That is, the prior distribution is not invariant to change of variables.</p>
<p>If, however, we chose an improper prior of <span class="math inline">\(g(\sigma) = 1/\sigma = \tau\)</span>, then we end up with <span class="math inline">\(g(\tau) = 1/\tau\)</span>, so the priors are consistent. It does not matter which parametrization we choose, <span class="math inline">\(\sigma\)</span> or <span class="math inline">\(\tau = 1/\sigma\)</span>, so long as the prior is <span class="math inline">\(1/\sigma\)</span> or <span class="math inline">\(1/\tau\)</span>, we get the same effect of the prior.</p>
<p><a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Harold Jeffreys</a> noticed this and discovered a way to make the priors invariant to change of coordinates. He developed what is now known as the <strong>Jeffreys prior</strong>, which is given by the square root of the determinant of the <strong>Fisher information matrix</strong>. If <span class="math inline">\(f(y\mid\theta)\)</span> is the likelihood (where <span class="math inline">\(\theta\)</span> here is a set of parameters), the Fisher information matrix is the negative expectation value of the matrix of second derivatives of the log-likelihood. That is, entry <span class="math inline">\(i, j\)</span> in the Fisher information matrix <span class="math inline">\(\mathcal{I}\)</span> is</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}_{ij}(\theta) = -\int\mathrm{d}y \,\frac{\partial^2 f(y\mid \theta)}{\partial \theta_i\partial \theta_j} \, f(y\mid \theta) \equiv -\mathrm{E}\left[\frac{\partial^2 \ln f(y\mid \theta)}{\partial \theta_i\partial \theta_j}\right],
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathrm{E}[\cdot]\)</span> denotes the expectation value over the likelihood. For ease of calculation later, it is useful to know that this is equivalent to</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}_{ij}(\theta) = \mathrm{E}\left[\left(\frac{\partial \ln f(y\mid \theta)}{\partial \theta_i}\right)\left(\frac{\partial \ln f(y\mid \theta)}{\partial \theta_j}\right)\right].
\end{align}\]</span></p>
<p>Written more succinctly, let <span class="math inline">\(\mathsf{B}_\theta\)</span> be the Hessian matrix, that is the matrix of partial derivatives of the the log likelihood.</p>
<p><span class="math display">\[\begin{aligned}
\begin{align}
\mathsf{B}_\theta = \begin{pmatrix}
\frac{\partial^2 \ln f}{\partial \theta_1^2} &amp;  \frac{\partial^2 \ln f}{\partial \theta_1 \partial \theta_2} &amp; \cdots \\
\frac{\partial^2 \ln f}{\partial \theta_2 \partial \theta_1} &amp;  \frac{\partial^2 \ln f}{\partial \theta_2^2} &amp; \cdots \\
\vdots &amp; \vdots &amp; \ddots
\end{pmatrix}
\end{align}.
\end{aligned}\]</span></p>
<p>Then,</p>
<p><span class="math display">\[\begin{align}
\mathcal{I}(\theta) = -\mathrm{E}\left[\mathsf{B}_\theta\right],
\end{align}\]</span></p>
<p>Due to its relation to the second derivatives of the likelihood function, the Fisher information matrix is related to the sharpness of a peak in the likelihood.</p>
<p>The Jeffreys prior is then</p>
<p><span class="math display">\[\begin{align}
g(\theta) \propto \sqrt{\mathrm{det}\, \mathcal{I}(\theta)}.
\end{align}\]</span></p>
<p>It can be shown that the determinant of the Fisher information matrix is strictly nonnegative, so that <span class="math inline">\(g(\theta)\)</span> as defined above is always real valued. To demonstrate that this choice of prior works to maintain the same functional form of priors under reparametrization, consider a reparametrization from <span class="math inline">\(\theta\)</span> to <span class="math inline">\(\phi\)</span>. By the multivariate change of variables formula,</p>
<p><span class="math display">\[\begin{align}
g(\phi) \propto \left|\mathrm{det}\,\mathsf{J}\right|g(\theta),
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{aligned}
\begin{align}
\mathsf{J} = \begin{pmatrix}
\frac{\partial \theta_1}{\partial \phi_1} &amp;  \frac{\partial \theta_1}{\partial \phi_2} &amp; \cdots \\
\frac{\partial \theta_2}{\partial \phi_1} &amp;  \frac{\partial \theta_2}{\partial \phi_2} &amp; \cdots \\
\vdots &amp; \vdots &amp; \ddots
\end{pmatrix}
\end{align}
\end{aligned}\]</span></p>
<p>is a matrix of derivatives, called the <a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobi matrix</a>. Using the fact that <span class="math inline">\(g(\theta) \propto \sqrt{\mathrm{det}\,\mathcal{I}(\theta)}\)</span> for a Jeffreys prior, we have</p>
<p><span class="math display">\[\begin{align}
g(\phi) \propto \left|\mathrm{det}\,\mathsf{J}\right|\,\sqrt{\mathrm{det}\,\mathcal{I}(\theta)}
= \sqrt{\left(\mathrm{det}\,\mathsf{J}\right)^2\,\mathrm{det}\,\mathcal{I}(\theta)}.
\end{align}\]</span></p>
<p>Because the product of determinants of a set of matrices is equal to the determinant of the product of the matrices, we can write this as</p>
<p><span class="math display">\[\begin{align}
g(\phi) \propto \sqrt{\mathrm{det}\left(\mathsf{J}\cdot \mathcal{I}(\theta)\cdot \mathsf{J}\right)} = \sqrt{\mathrm{det}\left(\mathsf{J}\cdot \mathrm{E}[\mathsf{B}_\theta] \cdot \mathsf{J}\right)}.
\end{align}\]</span></p>
<p>Because <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> are not functions of <span class="math inline">\(y\)</span>, and therefore <span class="math inline">\(\mathsf{J}\)</span> is also not a function of <span class="math inline">\(y\)</span> we may bring the Jacobi matrices into the expectation operation.</p>
<p><span class="math display">\[\begin{align}
g(\phi) \propto \sqrt{\mathrm{det}\,\mathrm{E}\left[\mathsf{J}\cdot \mathsf{B}_\theta \cdot \mathsf{J}\right]}.
\end{align}\]</span></p>
<p>We recognize the quantity <span class="math inline">\(\mathsf{J}\cdot \mathsf{B}_\theta \cdot \mathsf{J}\)</span> as having the same form as the multivariable chain rule for second derivatives. Thus, we are converting <span class="math inline">\(\mathsf{B}_\theta\)</span> from being a matrix of second derivatives with respect to <span class="math inline">\(\theta\)</span> to being a matrix of second derivatives with respect to <span class="math inline">\(\phi\)</span>. Thus,</p>
<p><span class="math display">\[\begin{align}
g(\phi) \propto \sqrt{\mathrm{det}\,\mathrm{E}\left[\mathsf{B}_\phi\right]} = \sqrt{\mathrm{det}\,\mathcal{I}(\phi)},
\end{align}\]</span></p>
<p>thereby demonstrating that a Jeffreys prior is invariant to change of parametrizations.</p>
</section>
<section id="example-jeffreys-priors" class="level2" data-number="26.3">
<h2 data-number="26.3" class="anchored" data-anchor-id="example-jeffreys-priors"><span class="header-section-number">26.3</span> Example Jeffreys priors</h2>
<p>Computing a Jeffreys prior can be difficult. It involves computing derivatives of the likelihood and then computing expectations by performing integrals. As models become more complicated, analytical results for Jeffreys priors become intractable, which is one of the arguments against using them. Nonetheless, for two common likelihoods, we can compute the Jeffreys priors. We will not show the calculations (they involve the tedious calculations I just mentioned), but will state the results.</p>
<ul>
<li>For a Normal likelihood, the Jeffreys prior is <span class="math inline">\(g(\sigma) \propto 1/\sigma\)</span>. That means that the priors for parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are independent and that parameter <span class="math inline">\(\mu\)</span> should have a uniform prior and that <span class="math inline">\(\sigma\)</span> has a prior that goes like the inverse of <span class="math inline">\(\sigma\)</span>. This is an example of a Jeffreys prior that is improper.</li>
<li>For a Binomial or Bernoulli likelihood, the Jeffreys prior for the parameter <span class="math inline">\(\theta\)</span>, which is the probability of success of a Bernoulli trial, is <span class="math inline">\(g(\theta) = 1/\pi\sqrt{\theta(1-\theta)}\)</span>, defined on the interval [0, 1]. This is a proper prior. Note that it is highly peaked at zero and at one. This suggests that the probability of success for a Bernoulli trial, <em>a priori</em>, is most likely very close to zero or one.</li>
</ul>
</section>
<section id="why-not-use-jeffreys-priors" class="level2" data-number="26.4">
<h2 data-number="26.4" class="anchored" data-anchor-id="why-not-use-jeffreys-priors"><span class="header-section-number">26.4</span> Why not use Jeffreys priors?</h2>
<p>Jeffreys priors are pleasing in that they deal with Fisher's criticisms. They guarantee that we get the same results, regardless of choice of parametrization of the likelihood. They are also not very <strong>informative</strong>, meaning that the prior has little influence over the posterior, leaving almost all of the influence to the likelihood. This is also pleasing because it gives a sense of a lack of bias. However, there are still several reasons why not to use Jeffreys priors.</p>
<ol type="1">
<li>They can be very difficult or impossible to derive for more complicated models.</li>
<li>They can be improper. When they are improper, the prior is not encoding prior knowledge using probability, since an improper prior cannot be a probability or probability density.</li>
<li>In the case of hierarchical models, which we will get to later in the term, use of Jeffreys priors can nefariously lead to improper <em>posteriors</em>! It is often difficult to discover that this is the case for a particular model without doing a very careful analysis.</li>
<li>They still do not really encode prior knowledge anyway. We still have the problem of a kinesin motor traveling at faster than the speed of light.</li>
</ol>
</section>
<section id="weakly-informative-priors" class="level2" data-number="26.5">
<h2 data-number="26.5" class="anchored" data-anchor-id="weakly-informative-priors"><span class="header-section-number">26.5</span> Weakly informative priors</h2>
<p>Remember, the prior probability distribution captures what we know about the parameter before we measure data. When coming up with a prior, I often like to sketch how I think the probability density or mass function of a parameter will look. This is directly encoding my prior knowledge using probability, which is what a prior is supposed to do by definition. When sketching the probability density function, though, I make sure that I draw the distribution broad enough that it covers all parameter values that are even somewhat reasonable. I limit its breadth to rule out absurd values, such as kinesin traveling faster than the speed of light. Such a prior is called a <strong>weakly informative prior</strong>.</p>
<p>To come up with the functional form, or better yet the name, of the prior distribution, I use the <a href="https://distribution-explorer.github.io/">Distribution Explorer</a> to find a distribution and parameter set that matches my sketch. If I have to choose between making the prior more peaked or broader, I opt for being broader. This is well-described in the useful <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Stan wiki on priors</a>, which says, “the loss in precision by making the prior a bit too weak (compared to the true population distribution of parameters or the current expert state of knowledge) is less serious than the gain in robustness by including parts of parameter space that might be relevant.”</p>
<p>I generally prefer to use weakly informative priors, mostly because they actually encode prior knowledge, separating the sublime from the ridiculous. In fact, we used weakly informative priors in the example of <em>C. elegans</em> egg lengths in the first part of this lecture. As we will see when we perform MCMC calculations, there are also practical advantages to using weakly informative priors. In general, prior choice can affect the second main task of Bayesian inference: making sense of the posterior. We will discuss these practical considerations when we start summarizing posteriors using Markov chain Monte Carlo.</p>
</section>
<section id="conjugate-priors" class="level2" data-number="26.6">
<h2 data-number="26.6" class="anchored" data-anchor-id="conjugate-priors"><span class="header-section-number">26.6</span> Conjugate priors</h2>
<p>We have <code class="interpreted-text" role="ref">discussed conjugate priors&lt;4. Conjugacy&gt;</code> in the context of plotting posteriors. Conjugate priors are useful because we can make sense of the posterior analytically; the posterior and prior are the same distribution, differing by the updated parametrization in the posterior. If it is convenient to use a conjugate prior to encode prior information as we have described in our discussion of weakly informative priors, you can do so. There are two difficulties that make this convenience rare in practice.</p>
<ol type="1">
<li>Only a few likelihoods have known conjugate priors. Even in cases where the conjugate is known, its probability density function can be a complicated function.</li>
<li>As soon as a model grows in complexity beyond one or two parameters, and certainly into hierarchy, conjugate priors are simply not available.</li>
</ol>
<p>Thus, conjugate priors, while conceptually pleasing and parametrizable into weakly informative priors, have limited practical use.</p>
</section>
<section id="the-bet-the-farm-method-of-specifying-weakly-informative-priors" class="level2" data-number="26.7">
<h2 data-number="26.7" class="anchored" data-anchor-id="the-bet-the-farm-method-of-specifying-weakly-informative-priors"><span class="header-section-number">26.7</span> The bet-the-farm method of specifying weakly informative priors</h2>
<p>Since we have decided that weakly informative priors are of greatest utility, I will share a technique I like to use for coming up with priors for positive continuous unbounded parameters, a commonly encountered situation. I like to take what I call the <a href="https://en.wiktionary.org/wiki/bet_the_farm">bet-the-farm</a> approach. The idiomatic term "to bet the farm" means to make a giant wager on an outcome. You could think instead about betting a year's salary. As an example, would you bet the farm that pigs cannot fly? I would be comfortable waging a year's salary (since I don't have a farm) that they do not. While I certainly hope the Los Angeles Football Club wins the MLS cup in the next few years, I would not be comfortable betting the farm on it.</p>
<p>The technique is best described through example. Let's say someone tells me about a new bacterium and I have to guess how long a single cell of that species is.</p>
<p>To make my guess, I start absurdly low. Certainly, the cell is bigger than of order nanometer, since that's the diameter of a strand of DNA. I would bet the farm (or a year's salary) on it. I would also bet the farm that it would be bigger than 10 nm without flinching. How about 100 nm? Well, I'm pretty sure that bacteria tend not to be smaller than 100 nm, but I don't think I'd bet the farm. I feel uneasy enough about that that I won't make that bet. So, I put 100 nm as the lower end of my guess.</p>
<p>Now, let's consider absurdly large sizes. I would bet the farm that it is less than a meter long. How about 10 cm? That's still gigantic, and I would bet the farm that it's smaller than that. How about 1 cm? Still gigantic. How about 1 mm? Well, this is still huge, but there is tremendous diversity among bacteria. I know there are eukaryotic cells this big (for example a <em>Xenopus</em> egg), so, even though I strongly suspect that bacterium would be smaller than 1 mm, I wouldn't bet a farm. So, 1 mm is my upper bound.</p>
<p>If we were coming up with an order-of-magnitude estimate, we would take the geometric mean of the high and low boundaries. In this case, we would get <span class="math inline">\(\sqrt{10^{-7}\cdot 10^{-3}} \text{ m} = 10^{-5}\text{ m} =\)</span> 10 µm, which, perhaps not surprisingly, is within an order of magnitude of "typical" bacterial size, for example of <em>E. coli</em>.</p>
<p>Notice that these order-of-magnitude type of estimates operates on a logarithmic scale. We estimated between <span class="math inline">\(10^{-7}\)</span> and <span class="math inline">\(10^{-3}\)</span> meters. So, for encoding a prior for the parameter, it is convenient to come up with the prior for the base-ten logarithm of the parameter instead (ignoring the mathematical absurdity with taking logarithms of quantities with units), and then transform the variable. In the bacterial size example, I could use a Normal distribution where 95% of the probability mass lies between <span class="math inline">\(-7\)</span> and <span class="math inline">\(-3\)</span>. The width of my range of reasonable values from the bet-the-farm approach is 4 log units, so if I choose a Normal distribution centered at <span class="math inline">\(-5\)</span> with scale parameter of 1, I capture this prior information. So, my prior for the bacterial length <span class="math inline">\(\ell\)</span> is</p>
<p><span class="math display">\[\begin{aligned}
\begin{align}
&amp;\log_{10} \ell \sim \text{Norm}(-5, 1),\\[1em]
&amp;\ell = 10^{\log_{10}\ell}.
\end{align}
\end{aligned}\]</span></p>
<p>Equivalently, since <span class="math inline">\(\ln 10 \approx 2.3\)</span>, we can write this as <span class="math inline">\(\ell \sim \text{LogNorm}(-2.3\cdot 5, 2.3)\)</span>.</p>
<p>To summarize the procedure for finding a prior for positive continuous unbounded parameter <span class="math inline">\(\theta\)</span> is as follows.</p>
<ol type="1">
<li>Start at absurdly low values for the parameter and work your way up to a value that you would be hesitant to bet the farm on. This is your low estimate, <span class="math inline">\(\theta_\mathrm{min}\)</span>.</li>
<li>Start at absurdly high values for the parameter and work your way down to a value that you would be hesitant to bet the farm on. This is your high estimate, <span class="math inline">\(\theta_\mathrm{max}\)</span>.</li>
<li>Determine the center of your two estimates on a logarithmic scale. This is the location parameter <span class="math inline">\(\mu_{10} = (\theta_\mathrm{max} + \theta_\mathrm{min})/2\)</span> for the Normal prior of <span class="math inline">\(\log_{10}\theta\)</span>.</li>
<li>Take the difference of the high to low estimates and divide it by four. The result is the scale parameter <span class="math inline">\(\sigma_{10} = (\theta_\mathrm{max} - \theta_\mathrm{min})/4\)</span> for the Normal prior of <span class="math inline">\(\log_{10}\theta\)</span>.</li>
<li>The prior for the base-ten logarithm of the parameter is then <span class="math inline">\(\log_{10}\theta \sim \text{Norm}(\mu_{10}, \sigma_{10})\)</span>. Equivalently, we can say that the prior is distributed as <span class="math inline">\(\theta \sim \text{LogNorm}(2.3\mu_{10}, 2.3\sigma_{10})\)</span>.</li>
</ol>
<p>The result is a broad distribution that contains all conceivable values of the parameters, as determined by you.</p>
<p>Earlier in this lesson, I came up with a Normal (not Log-Normal as I would get using the bet-the-farm approach) for the length of a <em>C. elegans</em> egg. This is because I had firm prior knowledge about <em>C. elegans</em> eggs; I have looked at countless of them, and they are about 50 µm long. For most cases where I do not have prior knowledge like that, I use the bet the farm approach.</p>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>I understand that I should be providing units on all parameters that I am specifying with numbers. I am not doing this here, nor throughout the course, to avoid notational clutter and to maintain focus on the modeling.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/datasai-summer-school\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../lessons/probability/bayesian_modeling.html" class="pagination-link" aria-label="Bayesian modeling and inference">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Bayesian modeling and inference</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../lessons/probability/stats_ml_ai.html" class="pagination-link" aria-label="What about machine learning and artificial intelligence?">
        <span class="nav-page-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">What about machine learning and artificial intelligence?</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p><em>DataSAI 2025</em></p>
</div>
  </div>
</footer>




</body></html>